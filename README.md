# GridWorld MDP: 值迭代与策略迭代\n\n本项目使用 Python 实现了一个简单的网格世界（GridWorld）环境，并在此环境中应用了两种经典的强化学习算法来求解马尔可夫决策过程（MDP）：值迭代（Value Iteration, VI）和策略迭代（Policy Iteration, PI）。\n\n## 项目简介\n\n网格世界是一个常见的用于演示和测试强化学习算法的环境。在这个项目中：\n\n-   **环境定义**: 使用 `GridWorld` 类来定义网格的大小、状态、动作、转移函数和奖励函数。\n    -   本实验使用 8x8 的网格。\n    -   包含特殊状态转移：\n        -   从 A=(0, 1) 到 A'=(7, 1)，奖励 +10。\n        -   从 B=(0, 3) 到 B'=(4, 3)，奖励 +5。\n        -   从 C=(0, 5) 到 C'=(2, 5)，奖励 +3。\n    -   智能体可以执行上、下、左、右四个动作。\n    -   移动到普通格子的代价（奖励）为 -1。\n    -   尝试移出边界会停留在原地，并受到 -1 的代价。\n-   **求解算法**: \n    -   实现了 `value_iteration` 函数，通过迭代更新状态价值函数来求解最优策略。\n    -   实现了 `policy_iteration` 函数，通过策略评估和策略改进交替进行，该函数内部调用 `policy_evaluation` 和 `policy_improvement`。\n-   **目标**: 比较在不同折扣因子 \(\gamma\)（0.9 和 0.6）下，值迭代和策略迭代收敛到最优策略所需的迭代次数。\n\n## 如何运行\n\n1.  **环境**: 确保您已安装 Python 和 NumPy 库。\n    ```bash\n    pip install numpy \n    ```\n2.  **执行脚本**: 在终端中运行主脚本文件：\n    ```bash\n    python gridworld_mdp.py\n    ```\n\n脚本将自动运行值迭代和策略迭代算法，分别针对 \(\gamma = 0.9\) 和 \(\gamma = 0.6\)，并打印出收敛所需的迭代次数。\n\n## 实验结果\n\n根据运行 `gridworld_mdp.py` 得到的输出：\n\n```\n===== gamma = 0.9 =====\n[Value Iteration] Converged in 133 iterations.\n[Policy Iteration] Stabilized in 5 policy improvement steps.\n===== gamma = 0.6 =====\n[Value Iteration] Converged in 29 iterations.\n[Policy Iteration] Stabilized in 4 policy improvement steps.\n===== Summary =====\nGamma = 0.9:\n  Value Iteration iterations: 133\n  Policy Iteration iterations: 5\nGamma = 0.6:\n  Value Iteration iterations: 29\n  Policy Iteration iterations: 4\n```\n\n### 结果分析\n\n-   **迭代次数**: \n    -   策略迭代在两种 \(\gamma\) 值下都仅需较少次数的**策略改进**步骤（分别为 5 次和 4 次）即可稳定。需要注意的是，每次策略改进内部都包含了一次完整的**策略评估**（本身可能需要多次迭代才能使价值函数收敛）。\n    -   值迭代所需的总迭代次数（价值函数收敛）相对较多（分别为 133 次和 29 次）。\n-   **折扣因子 \(\gamma\) 的影响**: \n    -   当 \(\gamma\) 从 0.9 降低到 0.6 时，两种算法的收敛速度都显著加快。这是因为较小的 \(\gamma\) 更注重短期奖励，价值函数的传播范围和依赖链条变短，从而更快收敛。\n\n## 定制与扩展\n\n-   您可以在 `gridworld_mdp.py` 文件末尾的 `if __name__ == \"__main__\":` 部分修改 `GridWorld` 的初始化参数，例如网格大小、特殊状态位置、奖励值、移动代价等。\n-   可以通过取消注释 `print_policy` 和 `print_value_function` 的调用来可视化最终的策略和价值函数。\n-   可以尝试其他折扣因子值来观察算法性能的变化。\n\n## 算法说明\n\n### 值迭代\n\n值迭代基于贝尔曼最优方程，通过迭代方式更新状态价值函数，直到收敛到最优价值函数。算法伪代码：\n\n1. 初始化 $V(s)=0$ 对所有状态 $s$\n2. 重复直到收敛：\n   - 对每个状态 $s$：\n     - $V(s) \leftarrow \max_a [R(s,a,s') + \gamma V(s')]$ \n   - 如果价值函数变化小于阈值 $\theta$，则停止 \n3. 输出最优策略 $\pi(s) = \arg\max_a [R(s,a,s') + \gamma V(s')]$ \n\n### 策略迭代\n\n策略迭代通过策略评估和策略改进交替进行，算法伪代码：\n\n1. 初始化任意策略 $\pi$\n2. 重复直到策略稳定：\n   - **策略评估**：计算当前策略 $\pi$ 下的价值函数 $V_\pi$\n   - **策略改进**：更新 $\pi(s) = \arg\max_a [R(s,a,s') + \gamma V_\pi(s')]$ \n   - 如果策略没有变化，则停止 \n3. 输出最优策略 $\pi$ 