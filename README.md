# GridWorld MDP 说明

本项目使用马尔可夫决策过程（MDP）方法（值迭代与策略迭代）来求解一个 8×8 的网格世界问题。

## 问题描述

- 网格大小：8×8。
- 代理可以在每个格子上做 4 个动作：上 (Up)、下 (Down)、左 (Left)、右 (Right)。
- 特殊位置及奖励：
  - **A** 位于 (0,1)，无论采取什么动作，都会将代理传送到 (7,1) 并获得奖励 +10。
  - **B** 位于 (0,3)，无论采取什么动作，都会将代理传送到 (4,3) 并获得奖励 +5。
  - **C** 位于 (0,5)，无论采取什么动作，都会将代理传送到 (2,5) 并获得奖励 +3。
- 普通移动的代价（step_cost）为 -1。即除了到达 A/B/C 时获得正向奖励外，每走一步都要付出 -1 的代价。
- 折扣因子 $\gamma$ 分别取 0.9 和 0.6 进行实验，观察不同折扣因子下的最优策略与价值函数。

## 主要文件说明

- **gridworld_mdp.py**  
  - `GridWorld` 类：定义了网格世界环境，包括状态空间、动作空间、特殊位置跳转逻辑及即时奖励计算等。
  - `value_iteration(env, gamma, theta)`：实现值迭代（Value Iteration）算法。
    - **参数**：
      - `env`: 网格环境 `GridWorld`
      - `gamma`: 折扣因子
      - `theta`: 收敛阈值（当两次迭代的价值函数差距小于该值时认为收敛）
    - **返回**：
      - `V`: 收敛后的状态价值函数
      - `policy`: 贪心策略（从价值函数中直接导出）
      - `iterations`: 值迭代的迭代次数
  - `policy_evaluation(env, policy, gamma, theta)`：对给定策略进行策略评估。
  - `policy_improvement(env, V, policy, gamma)`：在给定价值函数的情况下进行一次贪心策略改进。
  - `policy_iteration(env, gamma)`：实现策略迭代（Policy Iteration）算法。
    - **返回**：
      - `policy`: 收敛后的最优策略
      - `V`: 收敛策略对应的价值函数
      - `iterations`: 策略改进的次数
  - 若在命令行运行该脚本（`python gridworld_mdp.py`），会依次对 $\gamma = 0.9$ 和 $\gamma = 0.6$ 进行值迭代和策略迭代，并打印相应的策略、价值函数和迭代次数。

## 使用方法

1. 克隆或下载本仓库到本地。
2. 确保已安装 Python 3 环境。
3. 在终端（或命令行）中进入项目所在目录，执行：
   ```bash
   python gridworld_mdp.py
   ```
4. 代码会在控制台打印出如下信息：
   - 不同 $\gamma$ 值下，值迭代收敛的迭代次数，以及最终的最优策略与价值函数。
   - 不同 $\gamma$ 值下，策略迭代稳定所需的策略改进次数，以及最终的最优策略与价值函数。
   - 以及最终的对比总结。

## 运行结果示例

以下是项目中已经给出的运行结果（截取自题主提供的终端输出），可与本地运行时得到的结果相互对比：

### 当 $\gamma = 0.9$ 时

- **值迭代 (Value Iteration)**  
  - 收敛迭代次数：`133`
  - 最优策略 (部分示意)：
    ```
    +---+---+---+---+---+---+---+---+
    | > | * | < | * | < | * | < | < |
    +---+---+---+---+---+---+---+---+
    | ^ | ^ | ^ | < | < | ^ | ^ | ^ |
    ...
    ```
  - 最终价值函数 (部分示意)：
    ```
    +-------+-------+-------+-------+-------+-------+-------+-------+
    |   7.38 |   9.31 |   7.38 |   5.24 |   3.71 |   4.76 |   3.28 |   1.96 |
    +-------+-------+-------+-------+-------+-------+-------+-------+
    |   5.64 |   7.38 |   5.64 |   4.08 |   2.67 |   3.28 |   1.96 |   0.76 |
    ...
    ```

- **策略迭代 (Policy Iteration)**  
  - 稳定所需的策略改进次数：`5`
  - 最优策略与最终价值函数与值迭代结果相同。

### 当 $\gamma = 0.6$ 时

- **值迭代 (Value Iteration)**  
  - 收敛迭代次数：`29`
  - 最优策略 (部分示意)：
    ```
    +---+---+---+---+---+---+---+---+
    | > | * | < | * | < | * | < | < |
    +---+---+---+---+---+---+---+---+
    | ^ | ^ | ^ | ^ | ^ | ^ | ^ | ^ |
    ...
    ```
  - 最终价值函数 (部分示意)：
    ```
    +-------+-------+-------+-------+-------+-------+-------+-------+
    |   4.21 |   8.69 |   4.21 |   4.01 |   1.40 |   2.60 |   0.56 |  -0.66 |
    +-------+-------+-------+-------+-------+-------+-------+-------+
    |   1.53 |   4.21 |   1.53 |   1.40 |  -0.16 |   0.56 |  -0.66 |  -1.40 |
    ...
    ```

- **策略迭代 (Policy Iteration)**  
  - 稳定所需的策略改进次数：`4`
  - 最优策略与最终价值函数与值迭代结果一致。

### 结果汇总

最终的对比摘要（Summary）：
```
===== Summary =====
Gamma = 0.9:
  Value Iteration iterations: 133
  Policy Iteration iterations: 5
Gamma = 0.6:
  Value Iteration iterations: 29
  Policy Iteration iterations: 4
```

## 结论与讨论

- 当 $\gamma$ 较大（如 0.9）时，代理更关注长期回报，因此在计算最优策略和价值函数时需要更多迭代才能收敛（例如值迭代需要 133 步）。
- 当 $\gamma$ 较小（如 0.6）时，代理更关注短期回报，因此收敛速度相对更快（如值迭代只需 29 步）。
- 无论值迭代还是策略迭代，在本案例中都可以收敛到相同的最优策略与价值函数；只是二者在迭代效率上可能有所不同：
  - 值迭代需要在每次更新时遍历所有状态并对动作进行最大化，可能迭代步数更多。
  - 策略迭代交替进行“策略评估 + 策略改进”，有时在实际问题中会更快或更慢，视具体环境和参数而定。

